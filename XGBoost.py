# -*- coding: utf-8 -*-
"""Untitled48.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1evP_h0oneh-Dj6WIwLlYkI_JFwWy8Lnd
"""

# Importing necessary framework
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings("ignore")

# upload fie from github
url = "https://github.com/htcysl/Glioma-Grading-Clinical-and-Mutation-Fetuares/raw/main/TCGA_InfoWithGrade.csv"
df = pd.read_csv(url)

df.shape

df.info()

df.keys()

X, y =df.drop('Grade',axis=1), df[['Grade']]

X.head()

y.head()

# Split the data

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 1)

"""The important part: XGBoost comes with its own class for storing datasets called DMatrix. It is a highly optimized class for memory ans speed.That's why convertign datasets into this format is arequirement for the native XGBoost API :"""

import xgboost as xgb

# Convert data lables to categorical format
dtrain = xgb.DMatrix(X_train, label = y_train)
dtest = xgb.DMatrix(X_test,label =y_test)

"""The class accepts both the training features and the tables. To enables automatic encoding of Pandas category columns, we also set enable_categorical to True"""

# Define hyperparameters for XGBoost
params = {
         'objective' : 'binary:logistic',     # Binary classification problem
         'eval_metric' : 'logloss' ,          # Evaluation metric: Logarithmic loss
         'max_depth' : 2,                     # Maximum depth of a tree
         'eta' : 0.01,                         # Learning rate
         'subsample' : 0.8,                   # Subsample raton of the training data
         'colsample_bytree' : 0.8,            # Subsample ration of columns when constructing each tree
          'seed' : 42                         # Random seed
}

# Train the XGBoost model
num_round = 100  # Number of boosting rounds (iterations)
model = xgb.train(params, dtrain, num_round)

# Make predictions on the test set
y_pred_prob = model.predict(dtest)
y_pred = (y_pred_prob > 0.5).astype(int)

# Calculate accuracy and other metrics
accuracy = accuracy_score(y_test,y_pred)

print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:\n",classification_report)

from sklearn.metrics import mean_squared_error

rmse = mean_squared_error(y_test,y_pred_prob,squared = False)

print(f"RMSE of the base model: {rmse: .3f}")